@PhdThesis{Buschjaeger/2022,
  author    = {Sebastian Buschj{\"{a}}ger},
  title     = {Ensemble learning with discrete classifiers on small devices},
  url       = {http://hdl.handle.net/2003/41132},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/phd/dnb/Buschjager22.bib},
  school    = {Technical University of Dortmund, Germany},
  timestamp = {Fri, 20 Jan 2023 21:27:23 +0100},
  urn       = {urn:nbn:de:101:1-2022111802423335923878},
  year      = {2022},
  selected = {true},
  abbr = {PhD Thesis}
}

@inproceedings{Buschjaeger/etal/2022,
      title={Shrub Ensembles for Online Classification (to appear, accepted)}, 
      author={Sebastian Buschjäger and Sibylle Hess and Katharina Morik},
      year={2022},
      url = {https://arxiv.org/abs/2112.03723},
      arxiv = {2112.03723},
      abbr = {AAAI},
      selected = {true},
      booktitle = {Proceedings of the Thirty-Sixth {AAAI} Conference on Artificial Intelligence (AAAI-22)},
      publisher = { {AAAI} Press},
      selected = {true},
      slides = {aaai2022_slides.pdf},
      poster = {aaai2022_poster.pdf},
      video = {https://slideslive.com/embed/presentation/38976989?embed_parent_url=https%3A%2F%2Faaai-2022.virtualchair.net%2Fposter_aaai10816&embed_container_origin=https%3A%2F%2Faaai-2022.virtualchair.net&embed_container_id=presentation-video&auto_load=true&auto_play=false&zoom_ratio=&disable_fullscreen=false&locale=en&vertical_enabled=true&vertical_enabled_on_mobile=false&allow_hidden_controls_when_paused=false&fit_to_viewport=true&user_uuid=6693ffb9-6d22-4e46-8c19-6db37e50a729},
      code = {https://github.com/sbuschjaeger/se-online}
}

@article{Chen/etal/2022,
      title={Efficient Realization of Decision Trees for Real-Time Inference (to appear, accepted)}, 
      author={Kuan-Hsun Chen and Chia-Hui Hsu and Christian Hakert and Sebastian Buschj\"ager and Chao-Lin Lee and Jenq-Kuen Lee and Katharina Morik and Jian-Jia Chen},
      journal = {ACM Transactions on Embedded Computing Systems},
      year = {2022},
      abbr = {TECS},
      selected = {true}
}

@misc{Buschjaeger/morik/2021c,
      title={There is no Double-Descent in Random Forests}, 
      author={Sebastian Buschjäger and Katharina Morik},
      year={2021},
      eprint={2111.04409},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url = {https://arxiv.org/abs/2111.04409},
      arxiv = {2111.04409},
      abbr = {arXiv},
      selected = {true}
}

@misc{Buschjaeger/Morik/2021b,
      title={Improving the Accuracy-Memory Trade-Off of Random Forests Via Leaf-Refinement}, 
      author={Sebastian Buschjäger and Katharina Morik},
      year={2021},
      eprint={2110.10075},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url = {https://arxiv.org/abs/2110.10075},
      arxiv = {2110.10075},
      abbr = {arXiv},
      selected = {true}
}

@inproceedings{Buschjaeger/etal/2021a,
  author = {Buschj\"{a}ger, Sebastian and Philipp-Jan Honysz and Pfahler, Lukas and Morik, Katharina},
  title = {Very Fast Streaming Submodular Function Maximization},
  booktitle = {Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  publisher = {Springer},
  year = {2021},
  url={https://2021.ecmlpkdd.org/wp-content/uploads/2021/07/sub_178.pdf},
  slides = {ecml2021_slides.pdf},
  video = {https://slideslive.com/38963553/very-fast-streaming-submodular-function-maximization},
  selected = {true},
  abbr = {ECML}
}

@misc{Honysz/etal/2021b,
    title={Providing Meaningful Data Summarizations Using Exemplar-based Clustering in Industry 4.0}, 
    author={Philipp-Jan Honysz and Alexander Schulze-Struchtrup and Sebastian Buschjäger and Katharina Morik},
    year={2021},
    eprint={2105.12026},
    url = {https://arxiv.org/abs/2105.12026},
    arxiv = {2105.12026},
    abbr = {arXiv}
}

@misc{Honysz/etal/2021a,
  title={GPU-Accelerated Optimizer-Aware Evaluation of Submodular Exemplar Clustering}, 
  author={Philipp-Jan Honysz and Sebastian Buschjäger and Katharina Morik},
  year={2021},
  abstract = {The optimization of submodular functions constitutes a viable way to perform clustering. Strong approximation guarantees and feasible optimization w.r.t. streaming data make this clustering approach favorable. Technically, submodular functions map subsets of data to real values, which indicate how "representative" a specific subset is. Optimal sets might then be used to partition the data space and to infer clusters. Exemplar-based clustering is one of the possible submodular functions, but suffers from high computational complexity. However, for practical applications, the particular real-time or wall-clock run-time is decisive. In this work, we present a novel way to evaluate this particular function on GPUs, which keeps the necessities of optimizers in mind and reduces wall-clock run-time. To discuss our GPU algorithm, we investigated both the impact of different run-time critical problem properties, like data dimensionality and the number of data points in a subset, and the influence of required floating-point precision. In reproducible experiments, our GPU algorithm was able to achieve competitive speedups of up to 72x depending on whether multi-threaded computation on CPUs was used for comparison and the type of floating-point precision required. Half-precision GPU computation led to large speedups of up to 452x compared to single-precision, single-thread CPU computations.},
  url = {https://arxiv.org/pdf/2101.08763.pdf},
  arxiv = {2101.08763},
  code = {https://github.com/philippjh/exemcl},
  abbr = {arXiv}
}

@misc{Buschjaeger/etal/2021c,
  title={Bit Error Tolerance Metrics for Binarized Neural Networks}, 
  author={Sebastian Buschjäger and Jian-Jia Chen and Kuan-Hsun Chen and Mario Günzel and Katharina Morik and Rodion Novkin and Lukas Pfahler and Mikail Yayla},
  year={2021},
  arxiv = {2102.01344},
  url={https://arxiv.org/abs/2102.01344},
  abbr = {arXiv}
}

@article{Yayla/etal/2021,
  author={Yayla, Mikail and Buschjager, Sebastian and Gupta, Aniket and Chen, Jian-Jia and Henkel, Jorg and Morik, Katharina and Chen, Kuan-Hsun and Amrouch, Hussam},
  journal={IEEE Transactions on Computers}, 
  title={FeFET-based Binarized Neural Networks Under Temperature-dependent Bit Errors}, 
  abbr={TC},
  year={2021},
  pages={1-1},
  doi={10.1109/TC.2021.3104736},
  url = {https://ieeexplore.ieee.org/document/9513530}
}

@inproceedings{Buschjaeger/etal/2021a,
  author = {Buschj\"{a}ger, Sebastian and Chen, Jian-Jia and Chen, Kuan-Hsun and G{\"u}nzel, Mario and Hakert, Christian and Morik, Katharina and Novkin, Rodion and Pfahler, Lukas and Yayla, Mikail},
  title = {Margin-Maximization in Binarized Neural Networks for Optimizing Bit Error Tolerance},
  booktitle = {Proceedings of DATE 2021},
  year = {2021},
  abbr = {DATE},
  url = {https://ls12-www.cs.tu-dortmund.de/daes/media/documents/publications/downloads/2021dateyayla.pdf}
}

@article{Buschjager/Honysz/2020c,
	author = {Buschj{\"{a}}ger, Sebastian and Honysz, Philipp-jan and Morik, Katharina},
	doi = {10.1007/s41060-020-00238-w},
	isbn = {4106002000},
	issn = {2364-4168},
	journal = {International Journal of Data Science and Analytics},
	keywords = {Density estimation,Ensemble,Isolation forest,Outlier detection,Tree,density estimation,ensemble,isolation forest,outlier detection,tree},
	publisher = {Springer International Publishing},
	title = {{Randomized outlier detection with trees}},
	url = {https://doi.org/10.1007/s41060-020-00238-w},
  year = {2020},
  video = {https://www.youtube.com/watch?v=MOH6n8wiF7E},
  abbr = {JDSA}
}

@inproceedings{Buschjaeger/Honysz/2020a,
  author = {Buschj\"{a}ger, Sebastian and Honysz, Philipp-Jan and Morik, Katharina},
  title = {Generalized Isolation Forest: Some Theory and More Applications -- Extended Abstract},
  booktitle = {Proceedings 2020 IEEE 7th International Conference on Data Science and Advanced Analytics (DSAA 2020)},
  organization = {IEEE},
  year = {2020},
  abstract = {Isolation Forest is a popular outlier detection algorithm that isolates outlier
observations from regular observations by building multiple random decision trees. 
Multiple extensions enhance the original Isolation Forest algorithm including the Extended Isolation Forest which allows for non-rectangular splits and the SCiForest which improves the fitting of individual trees. All these approaches rate the outlierness of an observation by its average path-length. However, we find a lack of theoretical explanation on why these isolation-based algorithms offer such good practical performance.
In this paper, we present a theoretical framework that describes the effectiveness of isolation-based approaches from a distributional viewpoint. We show that these algorithms fit a mixture of distributions, where the average path length of an observation can be viewed as a (somewhat crude) approximation of the mixture coefficient. Using this framework, we derive the Generalized Isolation Forest (GIF) which also trains random trees, but combining them moves beyond using the average path-length. 
In an extensive evaluation of over \$350,000\$ experiments, we show that GIF outperforms the other methods on a variety of datasets while having comparable runtime.},
  abbr = {DSAA},
  video = {https://www.youtube.com/watch?v=MOH6n8wiF7E},
  url = {https://ieeexplore.ieee.org/abstract/document/9260007}
}

@article{Buschjaeger/Honysz/2020b,
  author = {Buschj\"{a}ger, Sebastian and Honysz, Philipp-Jan and Morik, Katharina},
  title = {Very Fast Streaming Submodular Function Maximization},
  year = {2020},
  abstract = {Data summarization has become a valuable tool in understanding even terabytes of data. Due to their compelling theoretical properties, submodular functions have been in the focus of summarization algorithms. These algorithms offer worst-case approximations guarantees to the expense of higher computation and memory requirements. However, many practical applications do not fall under this worst-case, but are usually much more well-behaved. In this paper, we propose a new submodular function maximization algorithm called ThreeSieves, which ignores the worst-case, but delivers a good solution in high probability. It selects the most informative items from a data-stream on the fly and maintains a provable performance on a fixed memory budget. In an extensive evaluation of more than 7000 experiments, we show that our algorithm outperforms current state-of-the-art algorithms and, at the same time, uses fewer resources. Last, we highlight a real-world use-case of our algorithm for data summarization in gamma-ray astronomy.},
  url = {https://arxiv.org/pdf/2010.10059.pdf},
  arxiv = {2010.10059},
  code = {https://github.com/sbuschjaeger/SubmodularStreamingMaximization},
  abbr = {arXiv}
}

@misc{Buschjaeger/etal/2020c,
  author = {Sebastian Buschj\"{a}ger and Lukas Pfahler and Katharina Morik},
  title = {Generalized Negative Correlation Learning for Deep Ensembling},
  year = {2020},
  arxiv = {2011.02952},
  url = {https://arxiv.org/pdf/2011.02952.pdf},
  abstract = {Ensemble algorithms offer state of the art performance in many machine learning applications. A common explanation for their excellent performance is due to the bias-variance decomposition of the mean squared error which shows that the algorithm's error can be decomposed into its bias and variance. Both quantities are often opposed to each other and ensembles offer an effective way to manage them as they reduce the variance through a diverse set of base learners while keeping the bias low at the same time. Even though there have been numerous works on decomposing other loss functions, the exact mathematical connection is rarely exploited explicitly for ensembling, but merely used as a guiding principle. In this paper, we formulate a generalized bias-variance decomposition for arbitrary twice differentiable loss functions and study it in the context of Deep Learning. We use this decomposition to derive a Generalized Negative Correlation Learning (GNCL) algorithm which offers explicit control over the ensemble's diversity and smoothly interpolates between the two extremes of independent training and the joint training of the ensemble. We show how GNCL encapsulates many previous works and discuss under which circumstances training of an ensemble of Neural Networks might fail and what ensembling method should be favored depending on the choice of the individual networks.},
  abbr = {arXiv}
}

@misc{Buschjaeger/etal/2020b,
  author = {Buschj\"{a}ger, Sebastian and Chen, Jian-Jia and Chen, Kuan-Hsun and G{\"u}nzel, Mario and Hakert, Christian and Morik, Katharina and Novkin, Rodion and Pfahler, Lukas and Yayla, Mikail},
  title = {Towards Explainable Bit Error Tolerance of Resistive RAM-Based Binarized Neural Networks},
  year = {2020},
  url = {https://www-ai.cs.tu-dortmund.de/PublicPublicationFiles/buschjaeger_etal_2020b.pdf}
}

@inproceedings{Buschjaeger/etal/2020a,
  author = {Buschj\"{a}ger, Sebastian and Pfahler, Lukas and Buss, Jens and Morik, Katharina and Rhode, Wolfgang},
  title = {On-Site Gamma-Hadron Separation with Deep Learning on FPGAs},
  booktitle = {Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  publisher = {Springer},
  year = {2020},
  url={https://link.springer.com/content/pdf/10.1007%2F978-3-030-67667-4_29.pdf},
  slides = {ecml2020_slides.pdf},
  video = {https://slideslive.com/38932415/onsite-gammahadron-separation-with-deep-learning-on-fpags},
  abbr = {ECML}
}

@inproceedings{Hakert/Yayla/2019a,
  author = {Hakert, Christian and Yayla, Mikail and Chen, Kuan-Hsun and Br\"uggen, Georg von der and Chen, Jian-Jia and Buschj\"ager, Sebastian and Morik, Katharina and Genssler, Paul R. and Bauer, Lars and Amrouch, Hussam and Henkel, J\"org},
  title = {Stack Usage Analysis for Efficient Wear Leveling in Non-Volatile Main Memory Systems},
  booktitle = {1st ACM/IEEE Workshop on Machine Learning for CAD (MLCAD)},
  year = {2019},
  abbr = {MLCAD},
  url = {https://ieeexplore.ieee.org/abstract/document/9142113}
}

@inproceedings{Buschjaeger/etal/2019a,
  author = {Buschj\"{a}ger, Sebastian and Liebig, Thomas and Morik, Katharina},
  title = {Gaussian Model Trees for Traffic Imputation},
  booktitle = {Proceedings of the International Conference on Pattern Recognition Applications and Methods (ICPRAM)},
  organization = {SciTePress},
  pages = {243 - 254},
  year = {2019},
  abstract = {Traffic congestion is one of the most pressing issues for smart cities. Information on traffic flow can be used to reduce congestion by predicting vehicle counts at unmonitored locations so that counter-measures can be applied before congestion appears. To do so pricy sensors must be distributed sparsely in the city and at important roads in the city center to collect road and vehicle information throughout the city in real-time. Then, Machine Learning models can be applied to predict vehicle counts at unmonitored locations. To be fault-tolerant and increase coverage of the traffic predictions to the suburbs, rural regions, or even neighboring villages, these Machine Learning models should not operate at a central traffic control room but rather be distributed across the city. Gaussian Processes (GP) work well in the context of traffic count prediction, but cannot capitalize on the vast amount of data available in an entire city. Furthermore, Gaussian Processes are a global and centra lized model, which requires all measurements to be available at a central computation node. Product of Expert (PoE) models have been proposed as a scalable alternative to Gaussian Processes. A PoE model trains multiple, independent GPs on different subsets of the data and weight individual predictions based on each experts uncertainty. These methods work well, but they assume that experts are independent even though they may share data points. Furthermore, PoE models require exhaustive communication bandwidth between the individual experts to form the final prediction. In this paper we propose a hierarchical Product of Expert model, which consist of multiple layers of small, independent and local GP experts. We view Gaussian Process induction as regularized optimization procedure and utilize this view to derive an efficient algorithm which selects independent regions of the data. Then, we train local expert models on these regions, so that each expert is responsible for a given region. The resulting algorithm scales well for large amounts of data and outperforms flat PoE models in terms of communication cost, model size and predictive performance. Last, we discuss how to deploy these local expert models onto small devices},
  url = {https://www.scitepress.org/PublicationsDetail.aspx?ID=g+tVIY+KNts=\&t=1},
  slides = {icpram2019_slides.pdf},
  abbr = {ICPRAM}
}

@inproceedings{Buschjaeger/2018a,
  author = {Buschjaeger, Sebastian and Chen, Kuan-Hsun and Chen, Jian-Jia and Morik, Katharina},
  title = {Realization of Random Forest for Real-Time  Evaluation through Tree Framing},
  booktitle = {The IEEE International Conference on Data Mining series (ICDM)},
  month = {November},
  year = {2018},
  abstract = {The optimization of learning has always been of particular concern for big data analytics. However, the ongoing integration of machine learning models into everyday life also demand the evaluation to be extremely fast and in real-time. Moreover, in the Internet of Things, the computing facilities that run the learned model are restricted. Hence, the  implementation of the model application must take the characteristics of the executing platform into account Although there exist some heuristics that optimize the code, principled approaches for fast execution of learned models are rare. In this paper, we introduce a method that optimizes the execution of Decision Trees (DT). Decision Trees form the basis of many ensemble methods, such as Random Forests (RF) or Extremely Randomized Trees (ET). For these methods to work best, trees should be as large as possible. This challenges the data and the instruction cache of modern CPUs and thus demand a more careful memory layout. Based on a probabilistic view of decision tree execution, we optimize the two most common implementation schemes of decision trees. We discuss the advantages and disadvantages of both implementations and present a theoretically well-founded memory layout which maximizes locality during execution in both cases. The method is applied to three computer architectures, namely ARM (RISC), PPC (Extended RISC) and Intel (CISC) and is automatically adopted to the specific architecture by a code generator. We perform over 1800 experiments on several real-world data sets and report an average speed-up of 2 to 4 across all three architectures by using the proposed memory layout. Moreover, we find that our implementation outperforms sklearn, which was used to train the models by a factor of 1500.},
  url = {https://ieeexplore.ieee.org/document/8594826},
  slides = {icdm2018_slides.pdf},
  poster = {icdm2018_poster.pdf},
  abbr = {ICDM}
}

@article{Morik/etal/2017a,
  author = {Morik, Katharina and Bockermann, Christian and Buschj\"{a}ger, Sebastian},
  title = {Big Data Science},
  booktitle = {German journal on Artificial Intelligence (KI 2017)},
  month = {12},
  number = {1},
  pages = {27--36},
  volume = {32},
  year = {2017},
  abstract = {In ever more disciplines, science is driven by data, which leads to data analytics becoming a primary skill for researchers. This includes the complete process from data acquisition at sensors, over pre-processing and feature extraction to the use and application of machine learning. Sensors here often produce a plethora of data that needs to be dealt with in near-realtime, which requires a combined effort of implementations at the hardware level to high-level design of data flows. In this paper we outline two use-cases of this wide span of data analysis for science in a real-world example in astroparticle physics. We outline a high-level design approach which is capable of defining the complete data flow from sensor hardware to final analysis.},
  url = {https://doi.org/10.1007/s13218-017-0522-8},
  abbr = {KI}
}

@article{Buschjaeger/Morik/2017b,
  author = {Buschj\"{a}ger, Sebastian and Morik, Katharina},
  title = {Decision Tree and Random Forest Implementations for Fast Filtering of Sensor Data},
  journal = {IEEE Transactions on Circuits and Systems I: Regular Papers},
  month = {1},
  number = {1},
  pages = {209--222},
  volume = {65-I},
  year = {2018},
  url = {https://doi.org/10.1109/TCSI.2017.2710627},
  abbr = {TCAS-I}
}

@inproceedings{Buschjaeger/Morik/2017a,
  author = {Buschj\"{a}ger, Sebastian and Morik, Katharina and Schmidt, Maik},
  title = {Summary Extraction on Data Streams in Embedded Systems},
  booktitle = {Proceedings of the ECML Workshop on IoT Large Scale Learning From Data Streams},
  publisher = {ceur-ws.org},
  year = {2017},
  url = {http://ceur-ws.org/Vol-1958/IOTSTREAMING3.pdf},
  slides = {iostreaming2017_slides.pdf},
  abbr = {IoTStreaming}
}

@mastersthesis{Buschjaeger/2016a,
  author = {Buschj\"{a}ger, Sebastian},
  title = {Online Gau{\ss}-Prozesse zur Regression auf FPGAs},
  school = {TU Dortmund},
  year = {2016},
  abbr = {MA},
  url = {https://www-ai.cs.tu-dortmund.de/PublicPublicationFiles/buschjaeger_2016a.pdf}
}


@inproceedings{Buschjaeger/etal/2015a,
  author = {Buschj\"{a}ger, Sebastian and Pfahler, Lukas and Morik, Katharina},
  title = {Discovering Subtle Word Relation in Large German Corpora},
  booktitle = {Proceedings of the 3rd Workshop on the Challenges in the Management of Large Corpora},
  year = {2015},
  url = {https://www-ai.cs.tu-dortmund.de/PublicPublicationFiles/buschjaeger_etal_2015a.pdf}
}
  
  @techreport{Morik/etal/2015a,
  author = {Morik, Katharina and Jung, Alexander and Weckwerth, Jan  and R{\"o}tner, Stefan and Hess, Sibylle  and Buschj\"{a}ger, Sebastian and Pfahler, Lukas},
  title = {Untersuchungen zur Analyse von deutschsprachigen Textdaten},
  institution = {Technische Universit\"{a}t Dortmund},
  month = {12},
  number = {2},
  year = {2015},
  url = {https://www-ai.cs.tu-dortmund.de/PublicPublicationFiles/morik_etal_2015a.pdf}
}



